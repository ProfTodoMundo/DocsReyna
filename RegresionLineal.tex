%____________________________________________________________________________________________________________________________________
\documentclass{beamer}
%____________________________________________________________________________________________________________________________________
\usepackage[T1]{fontenc}
\usepackage[ansinew]{inputenc}
%\usepackage[spanish]{babel}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx,graphics}
%\usepackage{multicol}
\usepackage{multicol}
\usepackage{hyperref}
%___________________________________________________________________________________________________________________%
\newtheorem{Def}{Definici\'on}[section]
\newtheorem{Ejem}{Ejemplo}[section]
\newtheorem{Teo}{Teorema}[section]
\newtheorem{Dem}{Demostraci\'on}[section]
\newtheorem{Note}{Nota}[section]
\newtheorem{Sol}{Soluci\'on}[section]
\newtheorem{Prop}{Proposici\'on}[section]
\newtheorem{Coro}{Corolario}[section]
%___________________________________________________________________________________________________________________
%
\newcommand{\rea}{\mathbb{R}}
\newcommand{\esp}{\mathbb{E}}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\indora}{\mbox{$1$\hspace{-0.8ex}$1$}}
%___________________________________________________________________________________________________________________
%______________________________________________________________________
%
%\title{Curso de Estad\'istica II
\title[Curso de Estad\'istica II \hspace{25mm} \insertframenumber/\inserttotalframenumber]{An\'alisis de Regresi\'on Lineal}
%} %aqu se pueden usar saltos de lnea \\
\subtitle{\textbf{Universidad Aut\'onoma de la Ciudad de M\'exico}\\
\textit{Casa Libertad}}
\author{Carlos E. Mart\'inez Rodr\'iguez
%\small \tiny{Presenta:Coord. Plantel Casa Libertad}\\
%\small \tiny{Organizador Carlos E Mart\'inez R.}\\
%\small\footnotesize{Informes: carlos.martinez@uacm.edu.mx}
}
\tiny
%\email{carlosmroder@hotmail.com}
\institute{carlos.martinez@uacm.edu.mx\\
Academia de Matem\'aticas - Modelaci\'on Matem\'atica\\
 Colegio de Ciencia y Tecnolog\'ia\\}
\date{Semestre  2019-II}
%___________________________________________________________________________________________________________________
%______________________________________________________________________
%
\pgfdeclareimage[height=0.5cm]{logo-izq}{LOGOUACM.png}
\pgfdeclareimage[height=0.5cm]{logo-der}{LOGOUACM.png}
\logo{\pgfuseimage{logo-der}} \setbeamertemplate{sidebar left}
{\logo{\pgfuseimage{logo-izq}}
\vfill %pone la imgen en la esquina inferior izquierda
\rlap{\hskip0.1cm\insertlogo} %inserta la imgen
\vskip15pt}


%\usetheme{Ilmenau}% esta bueno cal=7
%\usetheme{Warsaw} % me gusta cal=7
%\usetheme{Malmoe} %muy simple
%\usetheme{boxes}% muy chafa! cal=5
%\usetheme{Rochester} % me gusta!, calificación = 6, pues le faltan los pies de página.
%\usetheme{Szeged} %No me gusta
%\usetheme{AnnArbor} % demasiado colorida
%\usetheme{Copenhagen} % parecida a una anterior, es decir, pelas!
\usetheme{Dresden} % leve, cal=5
%\usetheme{Pittsburgh} %chafa!!!
%\usetheme{Luebeck}
%___________________________________________________________________________________________________________________
%
\begin{document}
\setbeamertemplate{navigation symbols}{}
%___________________________________________________________________________________________________________________
%
%1
\begin{frame}
  \maketitle
\end{frame}

%\section*{}
%\small
\begin{frame}
\tableofcontents
\end{frame}
%\begin{frame}
%Hola
%\end{frame}
%---------------------------------------------------------
\section{3. An\'alisis de Regresion Lineal (RL)}
%---------------------------------------------------------
\begin{frame}\frametitle{Descripci\'on}
\begin{Note}
\begin{itemize}
\item En muchos problemas hay dos o m\'as variables relacionadas, para medir el grado de relaci\'on se utiliza el \textbf{an\'alisis de regresi\'on}. 
\item Supongamos que se tiene una \'unica variable dependiente, $y$, y varias  variables independientes, $x_{1},x_{2},\ldots,x_{n}$.

\item  La variable $y$ es una varaible aleatoria, y las variables independientes pueden ser distribuidas independiente o conjuntamente. 

\end{itemize}

\end{Note}
\end{frame}

\subsection{3.1 Regresi\'on Lineal Simple (RLS)}
\begin{frame}\frametitle{RLS}
\begin{itemize}

\item A la relaci\'on entre estas variables se le denomina modelo regresi\'on de $y$ en $x_{1},x_{2},\ldots,x_{n}$, por ejemplo $y=\phi\left(x_{1},x_{2},\ldots,x_{n}\right)$, lo que se busca es una funci\'on que mejor aproxime a $\phi\left(\cdot\right)$.

\end{itemize}

Supongamos que de momento solamente se tienen una variable independiente $x$, para la variable de respuesta $y$. Y supongamos que la relaci\'on que hay entre $x$ y $y$ es una l\'inea recta, y que para cada observaci\'on de $x$, $y$ es una variable aleatoria.

El valor esperado de $y$ para cada valor de $x$ es
\begin{eqnarray}
E\left(y|x\right)=\beta_{0}+\beta_{1}x
\end{eqnarray}
$\beta_{0}$ es la ordenada al or\'igen y $\beta_{1}$ la pendiente de la recta en cuesti\'on, ambas constantes desconocidas. 

\end{frame}

\subsection{3.2 M\'etodo de M\'inimos Cuadrados}
\begin{frame}\frametitle{M\'inimos Cuadrados}
Supongamos que cada observaci\'on $y$ se puede describir por el modelo
\begin{eqnarray}\label{Modelo.Regresion}
y=\beta_{0}+\beta_{1}x+\epsilon
\end{eqnarray}
donde $\epsilon$ es un error aleatorio con media cero y varianza $\sigma^{2}$. Para cada valor $y_{i}$ se tiene $\epsilon_{i}$ variables aleatorias no correlacionadas, cuando se incluyen en el modelo \ref{Modelo.Regresion}, este se le llama \textit{modelo de regresi\'on lineal simple}.


Suponga que se tienen $n$ pares de observaciones $\left(x_{1},y_{1}\right),\left(x_{2},y_{2}\right),\ldots,\left(x_{n},y_{n}\right)$, estos datos pueden utilizarse para estimar los valores de $\beta_{0}$ y $\beta_{1}$. Esta estimaci\'on es por el \textbf{m\'etodos de m\'inimos cuadrados}.
\end{frame}


\begin{frame}\frametitle{M\'inimos Cuadrados}
Entonces la ecuaci\'on \ref{Modelo.Regresion} se puede reescribir como
\begin{eqnarray}\label{Modelo.Regresion.dos}
y_{i}=\beta_{0}+\beta_{1}x_{i}+\epsilon_{i},\textrm{ para }i=1,2,\ldots,n.
\end{eqnarray}
Si consideramos la suma de los cuadrados de los errores aleatorios, es decir, el cuadrado de la diferencia entre las observaciones con la recta de regresi\'on
\begin{eqnarray}
L=\sum_{i=1}^{n}\epsilon^{2}=\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1}x_{i}\right)^{2}
\end{eqnarray}
\end{frame}



\begin{frame}\frametitle{M\'inimos Cuadrados}
Para obtener los estimadores por m\'inimos cuadrados de $\beta_{0}$ y $\beta_{1}$, $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$, es preciso calcular las derivadas parciales con respecto a $\beta_{0}$ y $\beta_{1}$, igualar a cero y resolver el sistema de ecuaciones lineales resultante:
\begin{eqnarray*}
\frac{\partial L}{\partial \beta_{0}}=0\\
\frac{\partial L}{\partial \beta_{1}}=0\\
\end{eqnarray*}
evaluando en $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$, se tiene
\end{frame}
\begin{frame}\frametitle{M\'inimos Cuadrados}
\begin{eqnarray*}
-2\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right)&=&0\\
-2\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right)x_{i}&=&0
\end{eqnarray*}
simplificando
\begin{eqnarray*}
n\hat{\beta}_{0}+\hat{\beta}_{1}\sum_{i=1}^{n}x_{i}&=&\sum_{i=1}^{n}y_{i}\\
\hat{\beta}_{0}\sum_{i=1}^{n}x_{i}+\hat{\beta}_{1}\sum_{i=1}^{n}x_{i}^{2}&=&\sum_{i=1}^{n}x_{i}y_{i}
\end{eqnarray*}
\end{frame}

\begin{frame}\frametitle{M\'inimos Cuadrados}
Las ecuaciones anteriores se les denominan \textit{ecuaciones normales de m\'inimos cuadrados} con soluci\'on
\begin{eqnarray}\label{Ecs.Estimadores.Regresion}
\hat{\beta}_{0}&=&\overline{y}-\hat{\beta}_{1}\overline{x}\\
\hat{\beta}_{1}&=&\frac{\sum_{i=1}^{n}x_{i}y_{i}-\frac{1}{n}\left(\sum_{i=1}^{n}y_{i}\right)\left(\sum_{i=1}^{n}x_{i}\right)}{\sum_{i=1}^{n}x_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)^{2}}
\end{eqnarray}
entonces el modelo de regresi\'on lineal simple ajustado es
\begin{eqnarray}
\hat{y}=\hat{\beta}_{0}+\hat{\beta}_{1}x
\end{eqnarray}
\end{frame}
\begin{frame}\frametitle{M\'inimos Cuadrados}
Se intrduce la siguiente notaci\'on
\begin{eqnarray}
S_{xx}&=&\sum_{i=1}^{n}\left(x_{i}-\overline{x}\right)^{2}=\sum_{i=1}^{n}x_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)^{2}\\
S_{xy}&=&\sum_{i=1}^{n}y_{i}\left(x_{i}-\overline{x}\right)=\sum_{i=1}^{n}x_{i}y_{i}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)\left(\sum_{i=1}^{n}y_{i}\right)
\end{eqnarray}
y por tanto
\begin{eqnarray}
\hat{\beta}_{1}=\frac{S_{xy}}{S_{xx}}
\end{eqnarray}
\end{frame}
\subsection{3.3 Propiedades de los Estimadores $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$}
\begin{frame}\frametitle{Propiedades de los estimadores}
\begin{Note}\pause
\begin{itemize}
\item Las propiedades estad\'isticas de los estimadores de m\'inimos cuadrados son \'utiles para evaluar la suficiencia del modelo.

\item Dado que $\hat{\beta}_{0}$ y  $\hat{\beta}_{1}$ son combinaciones lineales de las variables aleatorias $y_{i}$, tambi\'en resultan ser variables aleatorias.
\end{itemize}
\end{Note}
A saber
\begin{eqnarray*}
E\left(\hat{\beta}_{1}\right)&=&E\left(\frac{S_{xy}}{S_{xx}}\right)=\frac{1}{S_{xx}}E\left(\sum_{i=1}^{n}y_{i}\left(x_{i}-\overline{x}\right)\right)
\end{eqnarray*}
\end{frame}


\begin{frame}\frametitle{Propiedades de los estimadores}
\begin{eqnarray*}
&=&\frac{1}{S_{xx}}E\left(\sum_{i=1}^{n}\left(\beta_{0}+\beta_{1}x_{i}+\epsilon_{i}\right)\left(x_{i}-\overline{x}\right)\right)\\
&=&\frac{1}{S_{xx}}\left[\beta_{0}E\left(\sum_{k=1}^{n}\left(x_{k}-\overline{x}\right)\right)+E\left(\beta_{1}\sum_{k=1}^{n}x_{k}\left(x_{k}-\overline{x}\right)\right)\right.\\
&+&\left.E\left(\sum_{k=1}^{n}\epsilon_{k}\left(x_{k}-\overline{x}\right)\right)\right]=\frac{1}{S_{xx}}\beta_{1}S_{xx}=\beta_{1}
\end{eqnarray*}
por lo tanto 
\begin{equation}\label{Esperanza.Beta.1}
E\left(\hat{\beta}_{1}\right)=\beta_{1}
\end{equation}
\end{frame}

\begin{frame}\frametitle{Propiedades de los estimadores}
\begin{Note}
Es decir, $\hat{\beta_{1}}$ es un estimador insesgado.
\end{Note}
Ahora calculemos la varianza:
\begin{eqnarray*}
V\left(\hat{\beta}_{1}\right)&=&V\left(\frac{S_{xy}}{S_{xx}}\right)=\frac{1}{S_{xx}^{2}}V\left(\sum_{k=1}^{n}y_{k}\left(x_{k}-\overline{x}\right)\right)\\
&=&\frac{1}{S_{xx}^{2}}\sum_{k=1}^{n}V\left(y_{k}\left(x_{k}-\overline{x}\right)\right)=\frac{1}{S_{xx}^{2}}\sum_{k=1}^{n}\sigma^{2}\left(x_{k}-\overline{x}\right)^{2}\\
&=&\frac{\sigma^{2}}{S_{xx}^{2}}\sum_{k=1}^{n}\left(x_{k}-\overline{x}\right)^{2}=\frac{\sigma^{2}}{S_{xx}}
\end{eqnarray*}

\end{frame}

\begin{frame}\frametitle{Propiedades de los estimadores}
por lo tanto
\begin{equation}\label{Varianza.Beta.1}
V\left(\hat{\beta}_{1}\right)=\frac{\sigma^{2}}{S_{xx}}
\end{equation}
\begin{Prop}
\begin{eqnarray*}
E\left(\hat{\beta}_{0}\right)&=&\beta_{0},\\
V\left(\hat{\beta}_{0}\right)&=&\sigma^{2}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right),\\
Cov\left(\hat{\beta}_{0},\hat{\beta}_{1}\right)&=&-\frac{\sigma^{2}\overline{x}}{S_{xx}}.
\end{eqnarray*}
\end{Prop}
\end{frame}

\begin{frame}\frametitle{Propiedades de los estimadores}
Para estimar $\sigma^{2}$ es preciso definir la diferencia entre la observaci\'on $y_{k}$, y el valor predecido $\hat{y}_{k}$, es decir
\begin{eqnarray*}
e_{k}=y_{k}-\hat{y}_{k},\textrm{ se le denomina \textbf{residuo}.}
\end{eqnarray*}
La suma de los cuadrados de los errores de los reisduos, \textit{suma de cuadrados del error}
\begin{eqnarray}
SC_{E}=\sum_{k=1}^{n}e_{k}^{2}=\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}
\end{eqnarray}
\end{frame}

\begin{frame}\frametitle{Propiedades de los estimadores}
sustituyendo $\hat{y}_{k}=\hat{\beta}_{0}+\hat{\beta}_{1}x_{k}$ se obtiene
\begin{eqnarray*}
SC_{E}&=&\sum_{k=1}^{n}y_{k}^{2}-n\overline{y}^{2}-\hat{\beta}_{1}S_{xy}=S_{yy}-\hat{\beta}_{1}S_{xy},\\
E\left(SC_{E}\right)&=&\left(n-2\right)\sigma^{2},\textrm{ por lo tanto}\\
\hat{\sigma}^{2}&=&\frac{SC_{E}}{n-2}=MC_{E}\textrm{ es un estimador insesgado de }\sigma^{2}.
\end{eqnarray*}
\end{frame}

%\end{document}
\subsection{3.4 Prueba de Hip\'otesis en RLS}
\begin{frame}\frametitle{Prueba de Hip\'otesis}
\begin{itemize}
\item Para evaluar la suficiencia del modelo de regresi\'on lineal simple, es necesario lleva a cabo una prueba de hip\'otesis respecto de los par\'ametros del modelo as\'i como de la construcci\'on de intervalos de confianza.

\item Para poder realizar la prueba de hip\'otesis sobre la pendiente y la ordenada al or\'igen de la recta de regresi\'on es necesario hacer el supuesto de que el error $\epsilon_{i}$ se distribuye normalmente, es decir $\epsilon_{i} \sim N\left(0,\sigma^{2}\right)$.

\end{itemize}

\end{frame}

\begin{frame}\frametitle{Prueba de Hip\'otesis}
Suponga que se desea probar la hip\'otesis de que la pendiente es igual a una constante, $\beta_{0,1}$ las hip\'otesis Nula y Alternativa son:
\begin{centering}
\begin{itemize}
\item[$H_{0}$: ] $\beta_{1}=\beta_{1,0}$,

\item[$H_{1}$: ]$\beta_{1}\neq\beta_{1,0}$.

\end{itemize}
donde dado que las $\epsilon_{i} \sim N\left(0,\sigma^{2}\right)$, se tiene que $y_{i}$ son variables aleatorias normales $N\left(\beta_{0}+\beta_{1}x_{1},\sigma^{2}\right)$. 
\end{centering}
De las ecuaciones (\ref{Ecs.Estimadores.Regresion}) se desprende que $\hat{\beta}_{1}$ es combinaci\'on lineal de variables aleatorias normales independientes, es decir, $\hat{\beta}_{1}\sim N\left(\beta_{1},\sigma^{2}/S_{xx}\right)$, recordar las ecuaciones (\ref{Esperanza.Beta.1}) y (\ref{Varianza.Beta.1}).
\end{frame}

\begin{frame}\frametitle{Prueba de Hip\'otesis}
Entonces se tiene que el estad\'istico de prueba apropiado es
\begin{equation}\label{Estadistico.Beta.1}
t_{0}=\frac{\hat{\beta}_{1}-\hat{\beta}_{1,0}}{\sqrt{MC_{E}/S_{xx}}}
\end{equation}
que se distribuye $t$ con $n-2$ grados de libertad bajo $H_{0}:\beta_{1}=\beta_{1,0}$. Se rechaza $H_{0}$ si 
\begin{equation}\label{Zona.Rechazo.Beta.1}
t_{0}|>t_{\alpha/2,n-2}.
\end{equation}

\end{frame}

\begin{frame}\frametitle{Prueba de Hip\'otesis}
Para $\beta_{0}$ se puede proceder de manera an\'aloga para
\begin{itemize}
\item[$H_{0}:$] $\beta_{0}=\beta_{0,0}$,
\item[$H_{1}:$] $\beta_{0}\neq\beta_{0,0}$,
\end{itemize}
con $\hat{\beta}_{0}\sim N\left(\beta_{0},\sigma^{2}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)\right)$, por lo tanto
\begin{equation}\label{Estadistico.Beta.0}
t_{0}=\frac{\hat{\beta}_{0}-\beta_{0,0}}{MC_{E}\left[\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right]},
\end{equation}
con el que rechazamos la hip\'otesis nula si
\begin{equation}\label{Zona.Rechazo.Beta.0}
t_{0}|>t_{\alpha/2,n-2}.
\end{equation}

\end{frame}


\begin{frame}\frametitle{Prueba de Hip\'otesis}
\begin{itemize}
\item No rechazar $H_{0}:\beta_{1}=0$ es equivalente a decir que no hay relaci\'on lineal entre $x$ y $y$.
\item Alternativamente, si $H_{0}:\beta_{1}=0$ se rechaza, esto implica que $x$ explica la variabilidad de $y$, es decir, podría significar que la l\'inea recta esel modelo adecuado.
\end{itemize}
El procedimiento de prueba para $H_{0}:\beta_{1}=0$ puede realizarse de la siguiente manera:
\begin{eqnarray*}
S_{yy}=\sum_{k=1}^{n}\left(y_{k}-\overline{y}\right)^{2}=\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}+\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}
\end{eqnarray*}

\end{frame}


\begin{frame}\frametitle{Prueba de Hip\'otesis}
\begin{eqnarray*}
S_{yy}&=&\sum_{k=1}^{n}\left(y_{k}-\overline{y}\right)^{2}=\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}+\hat{y}_{k}-\overline{y}\right)^{2}\\
&=&\sum_{k=1}^{n}\left[\left(\hat{y}_{k}-\overline{y}\right)+\left(y_{k}-\hat{y}_{k}\right)\right]^{2}\\
&=&\sum_{k=1}^{n}\left[\left(\hat{y}_{k}-\overline{y}\right)^{2}+2\left(\hat{y}_{k}-\overline{y}\right)\left(y_{k}-\hat{y}_{k}\right)+\left(y_{k}-\hat{y}_{k}\right)^{2}\right]\\
&=&\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}+2\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)\left(y_{k}-\hat{y}_{k}\right)+\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}
\end{eqnarray*}
\end{frame}


\begin{frame}\frametitle{Prueba de Hip\'otesis}
\begin{eqnarray*}
&&\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)\left(y_{k}-\hat{y}_{k}\right)=\sum_{k=1}^{n}\hat{y}_{k}\left(y_{k}-\hat{y}_{k}\right)-\sum_{k=1}^{n}\overline{y}\left(y_{k}-\hat{y}_{k}\right)\\
&=&\sum_{k=1}^{n}\hat{y}_{k}\left(y_{k}-\hat{y}_{k}\right)-\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)\\
&=&\sum_{k=1}^{n}\left(\hat{\beta}_{0}+\hat{\beta}_{1}x_{k}\right)\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)-\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)
\end{eqnarray*}
\end{frame}

\begin{frame}\frametitle{Prueba de Hip\'otesis}
\begin{eqnarray*}
&=&\sum_{k=1}^{n}\hat{\beta}_{0}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)+\sum_{k=1}^{n}\hat{\beta}_{1}x_{k}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)\\
&-&\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)\\
&=&\hat{\beta}_{0}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)+\hat{\beta}_{1}\sum_{k=1}^{n}x_{k}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)\\
&-&\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)=0+0+0=0.
\end{eqnarray*}
\end{frame}

\begin{frame}\frametitle{Prueba de Hip\'otesis}
Por lo tanto, efectivamente se tiene
\begin{equation}\label{Suma.Total.Cuadrados}
S_{yy}=\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}+\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2},
\end{equation}
donde se hacen las definiciones
\begin{eqnarray}
SC_{E}&=&\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}\cdots\textrm{Suma de Cuadrados del Error}\\
SC_{R}&=&\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}\cdots\textrm{ Suma de Regresi\'on de Cuadrados}
\end{eqnarray}
\end{frame}

\begin{frame}\frametitle{Prueba de Hip\'otesis}
Por lo tanto la ecuaci\'on (\ref{Suma.Total.Cuadrados}) se puede reescribir como 
\begin{equation}\label{Suma.Total.Cuadrados.Dos}
S_{yy}=SC_{R}+SC_{E}
\end{equation}
recordemos que $SC_{E}=S_{yy}-\hat{\beta}_{1}S_{xy}$
\begin{eqnarray*}
S_{yy}&=&SC_{R}+\left( S_{yy}-\hat{\beta}_{1}S_{xy}\right)\\
S_{xy}&=&\frac{1}{\hat{\beta}_{1}}SC_{R}
\end{eqnarray*}
$S_{xy}$ tiene $n-1$ grados de libertad y $SC_{R}$ y $SC_{E}$ tienen 1 y $n-2$ grados de libertad respectivamente.
\end{frame}

\begin{frame}\frametitle{Prueba de Hip\'otesis}													
\begin{Prop}
\begin{equation}
E\left(SC_{R}\right)=\sigma^{2}+\beta_{1}S_{xx}
\end{equation}
adem\'as, $SC_{E}$ y $SC_{R}$ son independientes.
\end{Prop}
Recordemos que $\hat{\beta}_{1}=\frac{S_{xy}}{S_{xx}}$. Para $H_{0}:\beta_{1}=0$ verdadera,
\begin{eqnarray*}
F_{0}=\frac{SC_{R}/1}{SC_{E}/(n-2)}=\frac{MC_{R}}{MC_{E}}
\end{eqnarray*}
se distribuye $F_{1,n-2}$, y se rechazar\'ia $H_{0}$ si $F_{0}>F_{\alpha,1,n-2}$.
\end{frame}

\begin{frame}\frametitle{Prueba de Hip\'otesis}													
El procedimiento de prueba de hip\'otesis puede presentarse como la tabla de an\'alisis de varianza siguiente\medskip

\begin{tabular}{lcccc}\hline
Fuente de & Suma de  &  Grados de  & Media  & $F_{0}$ \\ 
 variaci\'on & Cuadrados & Libertad & Cuadr\'atica & \\\hline\pause
 Regresi\'on & $SC_{R}$ & 1 & $MC_{R}$  & $MC_{R}/MC_{E}$\\\pause
 Error Residual & $SC_{E}$ & $n-2$ & $MC_{E}$ & \\\hline\pause
 Total & $S_{yy}$ & $n-1$ & & \\\hline
\end{tabular} 
\end{frame}

\begin{frame}\frametitle{Prueba de Hip\'otesis}													
La prueba para la significaci\'on de la regresi\'on puede desarrollarse bas\'andose en la expresi\'on (\ref{Estadistico.Beta.1}), con $\hat{\beta}_{1,0}=0$, es decir\pause
\begin{equation}\label{Estadistico.Beta.1.Cero}
t_{0}=\frac{\hat{\beta}_{1}}{\sqrt{MC_{E}/S_{xx}}}
\end{equation}
Elevando al cuadrado ambos t\'erminos:
\begin{eqnarray*}
t_{0}^{2}=\frac{\hat{\beta}_{1}^{2}S_{xx}}{MC_{E}}=\frac{\hat{\beta}_{1}S_{xy}}{MC_{E}}=\frac{MC_{R}}{MC_{E}}
\end{eqnarray*}
Observar que $t_{0}^{2}=F_{0}$, por tanto la prueba que se utiliza para $t_{0}$ es la misma que para $F_{0}$.

\end{frame}

%\end{document}
\subsection{Estimaci\'on de Intervalos en RLS}
\begin{frame}\frametitle{Intervalos de Confianza}
\begin{itemize}
\item Adem\'as de la estimaci\'on puntual para los par\'ametros $\beta_{1}$ y $\beta_{0}$, es posible obtener estimaciones del intervalo de confianza de estos par\'ametros.\pause

\item El ancho de estos intervalos de confianza es una medida de la calidad total de la recta de regresi\'on.

\end{itemize}


\end{frame}

\begin{frame}\frametitle{Intervalos de Confianza}
Si los $\epsilon_{k}$ se distribuyen normal e independientemente, entonces
\begin{eqnarray*}
\begin{array}{ccc}
\frac{\left(\hat{\beta}_{1}-\beta_{1}\right)}{\sqrt{\frac{MC_{E}}{S_{xx}}}}&y &\frac{\left(\hat{\beta}_{0}-\beta_{0}\right)}{\sqrt{MC_{E}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)}}
\end{array}
\end{eqnarray*}
se distribuyen $t$ con $n-2$ grados de libertad.\pause Por tanto un intervalo de confianza de $100\left(1-\alpha\right)\%$ para $\beta_{1}$ est\'a dado por
\end{frame}
\begin{frame}\frametitle{Intervalos de Confianza}
\begin{eqnarray}
\hat{\beta}_{1}-t_{\alpha/2,n-2}\sqrt{\frac{MC_{E}}{S_{xx}}}\leq \beta_{1}\leq\hat{\beta}_{1}+t_{\alpha/2,n-2}\sqrt{\frac{MC_{E}}{S_{xx}}}.
\end{eqnarray}\pause
De igual manera, para $\beta_{0}$ un intervalo de confianza al $100\left(1-\alpha\right)\%$ es
\small{
\begin{eqnarray}
\begin{array}{l}
\hat{\beta}_{0}-t_{\alpha/2,n-2}\sqrt{MC_{E}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)}\leq\beta_{0}\leq\hat{\beta}_{0}+t_{\alpha/2,n-2}\sqrt{MC_{E}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)}
\end{array}
\end{eqnarray}}
\end{frame}
\end{document}
\begin{frame}\frametitle{Intervalos de Confianza}

\end{frame}
\begin{frame}\frametitle{Intervalos de Confianza}

\end{frame}

%\subsection{Predicci\'on}
\begin{frame}

\end{frame}


%\subsection{Bondad de Ajuste del Modelo}
\begin{frame}

\end{frame}


%\subsection{An\'alisis Residual}
\begin{frame}

\end{frame}


%\subsection{Prueba de falta de ajuste}
\begin{frame}

\end{frame}


%\subsection{Coeficiente de Determinaci\'on}
\begin{frame}

\end{frame}


%\subsection{Correlaci\'on}
\begin{frame}

\end{frame}


\end{document}
